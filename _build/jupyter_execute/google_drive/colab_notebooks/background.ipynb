{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqKBWVgJeFg5"
   },
   "source": [
    "# GIS and remote sensing introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZk51TbtrkBu"
   },
   "source": [
    "## Airborne and spaceborne imagery introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YFAlIIyRWrq"
   },
   "source": [
    "**Click on the image below** to watch a video by NEON where you'll learn:\n",
    "*   What is the difference between \"spectral resolution\" and \"spatial resolution\"? To learn more about spectral resolution, check out [this NEON tutorial explaining spectral bands, resolution, and FWHM](https://www.neonscience.org/resources/learning-hub/tutorials/hyper-spec-intro)\n",
    "*   What do scientists do with \"spectral signatures\"?\n",
    "*   What does the spectral signature of healthy vegetation look like?\n",
    "*   What is an \"imaging spectrometer\"?\n",
    "*   What is the difference between multi- and hyper-spectral imagery?\n",
    "\n",
    "[![NEON Youtube Channel](https://drive.google.com/uc?export=view&id=15l3G-8lKg8GUOgZG8EBz5VHqmusrI8Qa)](https://youtu.be/3iaFzafWJQE \"Hyperspectral remote sensing data\")\n",
    "\n",
    "**Click on the image below** to watch a video by the USGS showing how Landsat8 collects data globally over a 16 day period:\n",
    "\n",
    "[![USGS Youtube video](https://drive.google.com/uc?export=view&id=17QwwrZIfhWhnYpz2_UI1RwrcDRWLml7p)](https://www.youtube.com/watch?v=yPF2jpjB3Qw)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM1vPTdnePjG"
   },
   "source": [
    "## Radiance and reflectance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfc-N3Mseam_"
   },
   "source": [
    "Different light sources emit light at different wavelengths. In this image, we can see the brightness of light emitted by the sun, versus by a flourescent light bulb for visibles wavelengths.\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1MKiI2sqiFoaiAx4-HR9Hn556onsp44Xq' width=400> Image credit: [Webb Space Telescope](https://webbtelescope.org/images)\n",
    "\n",
    "After light is emitted from a source, objects interact with that light in different ways. As shown in the image below, light can be absorbed, pass through, or bounce off materials. When we're looking at objects, we see the light that gets reflected off their surfaces.  The light that is reflected back from objects is related to the brightness of light that reaches those objects. So, if you were in a room where the only source of light is the flourescent light bulb depicted in the figure above, you wouldn't expect to have light reflected off of surface with a wavelength of 400nm, because no light is being emitted in the room at 400nm.\n",
    "<br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1miUBgpT6otacOJNOFGR4Fdp-vIyY9bkM' width=400> Image credit: [Webb Space Telescope](https://webbtelescope.org/images)\n",
    "\n",
    "\n",
    "Light from the sun is reflected off of cloud's and Earth's surface. Some light is also absorbed by Earth's atmosphere.\n",
    "<br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1LvGxVdkBSTmspFfiBcV8YLr4GB8xwLGU' width=400> Image credit: [NASA](https://science.nasa.gov/ems/01_intro)\n",
    "\n",
    "\n",
    "Earth's atmosphere is opaque to certain wavelengths, while letting others pass through. When the sensor on the NEON AOP flight or on a satellite measures the light bouncing off of Earth's surface, it is capturing the light scattered by Earth's atmosphere, clouds, and neighboring pixels, along with light reflected off our our target pixel. This measurement is called \"radiance\".\n",
    "\n",
    "<br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1mRzcH-JuAeRWs4ELbI2sX664b9lCIFok' width=600> Image credit: [NASA](https://science.nasa.gov/ems/01_intro)\n",
    "\n",
    "Because the brighness of light hitting the Earth's surface at different wavelengths is so different and because of these scattering effects from the atmosphere and neighboring pixels, as ecologists, we tend to apply atmospheric and topographic corrections to the at-sensor radiance. In this process we convert from radiance (raw energy reaching our imaging spectrometer at different wavelengths) to reflectance (fraction of light that is reflected).\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1nDfgqO5plO5ywkcA2gY6G3I0fRJ40uuz' width=600>\n",
    "Image credit: [Thompson et al. 2019](https://www.researchgate.net/publication/326581977_Retrieval_of_Atmospheric_Parameters_and_Surface_Reflectance_from_Visible_and_Shortwave_Infrared_Imaging_Spectroscopy_Data)\n",
    "\n",
    "These reflectance data can give us really interesting ecological information. As shown in the image below, soil, different vegetation types, and water have completely different spectral signatures. <br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1gqHy1ySyAKfUeeyykP5rE9PvRhOBmH0f' width=400> Image credit: [NASA](https://science.nasa.gov/ems/01_intro)\n",
    "\n",
    "And, when we look at the spectral signatures of different vegetation types, we notices some interesting features, including chlorophyll and water absorption bands.<br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1vCj_UtLDbiveY_Oyxw_sO7NUACug3DOX' width=400> Image credit: [NASA](https://science.nasa.gov/ems/01_intro)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtXPoDaxehD-"
   },
   "source": [
    "## Projections and CRS\n",
    "\n",
    "QGIS (a free, open-source mapping software) has an introduction to map projections and coordinate reference systems (CRS). In their introduction, they define:\n",
    "> **Map projections** try to portray the surface of the earth, or a portion of the earth, on a flat piece of paper or computer screen. In layman’s term, map projections try to transform the earth from its spherical shape (3D) to a planar shape (2D).<br>\n",
    "> **A coordinate reference system (CRS)** then defines how the two-dimensional, projected map in your GIS relates to real places on the earth. The decision of which map projection and CRS to use depends on the regional extent of the area you want to work in, on the analysis you want to do, and often on the availability of data.\n",
    "\n",
    "You can read more on their website [here](https://docs.qgis.org/3.34/en/docs/gentle_gis_introduction/coordinate_reference_systems.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iWQKKW4ep_7"
   },
   "source": [
    "## Vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs4nXwvveetO"
   },
   "source": [
    "### Overview\n",
    "Types of vector data:\n",
    "- Points: single coordinate pair (ex: location of trapped small mammals)\n",
    "- Lines: Usually ordered set of coordinate pairs (ex: roads)\n",
    "- Polygons: Usually ordered set of coordinate pairs forming a closed polygon (ex: boundary of a study region)\n",
    "\n",
    "In the figure below, we can see a diagram of these 3 types of vector data. This figure also shows the extent of each dataset.\n",
    "\n",
    "<br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1eAVt4qEQSv9mhU-3DIMh1SuI8TY6JJc0' width=400> Image credit: [NEON](https://www.neonscience.org/resources/learning-hub/tutorials/dc-raster-data-r)\n",
    "\n",
    "\n",
    "In addition to having a geometry (ususally represented as coordinate(s)), vector data have extents, a projection, and attributes. Attributes can be seen in an attribute table, like in the figure shown below.\n",
    "<br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1L6qZIN_YxC7NJgHgSiSrX6uF2CVM-hbc' width=400> Image credit: [NEON](https://www.neonscience.org/resources/learning-hub/tutorials/dc-raster-data-r)\n",
    "\n",
    "In Google Earth Engine, each individual point, line, or polygon is called a Feature, and a grouping of Features is called a FeatureCollection. This is what the FeatureCollection data structure looks like on Google Earth Engine in Colab. This FeatureCollection has 243 Features, each of which has 8 attributes (called properties) and 1 geometry. This FeatureCollection also has a projection that isn't being shown in the printout here. But, its projection can be accessed using the .projection() method.\n",
    "<br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1OK6o6AQkd9ZIC-RbgWAxgfrt3VLd5C1l' width=400>\n",
    "\n",
    "### Common file formats\n",
    "- Shapefiles\n",
    "- KML\n",
    "- GeoJSON\n",
    "\n",
    "\n",
    "### Common Python packages for vector data\n",
    "- Geopandas\n",
    "- osgeo.ogr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aearvVHhehr1"
   },
   "source": [
    "## Raster data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nl1isVUvefYd"
   },
   "source": [
    "### Overview\n",
    "\n",
    "Raster data are gridded data. They have an extent and spatial resolution. The NEON AOP data have a spatial resolution of 1 meter, because each pixel is a 1m by 1m square. <br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1eASwE9UHDpKJziGK6SKdOiuE0anYo_rE' width=400>\n",
    "\n",
    "Raster data contain:\n",
    "- A data array with 3 dimensions: number of pixels wide, number of pixels tall, number of bands (although the dimensions are not always in that order). For example, if you are looking at the data array for a fire severity band, each value in the data array will represent the fire severity at that pixel.\n",
    "- The geotransform, which is a list of 6 values that allows us to know the size of each pixel and where the raster should be placed on the Earth (descriptions from the [GDAL documentation](https://gdal.org/tutorials/geotransforms_tut.html)).\n",
    "  *   GT(0) x-coordinate of the upper-left corner of the upper-left pixel.\n",
    "  *   GT(1) West-East pixel resolution / pixel width.\n",
    "  *   GT(2) row rotation (typically zero).\n",
    "  *   GT(3) y-coordinate of the upper-left corner of the upper-left pixel.\n",
    "  *   GT(4) column rotation (typically zero).\n",
    "  *   GT(5) North-South pixel resolution / pixel height (negative value for a north-up image).\n",
    "- A projection\n",
    "\n",
    "\n",
    "\n",
    "In Google Earth Engine, each raster layer is called an Image. A group of Images is called an ImageCollection. Each Image has bands, attributes (called properties), a projection, and (hidden from us in this printout) geographic information for the extent and spatial resolution of the raster. This ImageCollection has 93 Images, each of which has 1 band (Severity) and 7 attributes. These Images also have text describing the CRS and projection used for this Image's coordinates.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1MwHYtVv3dZsSXnylXisP16RFfHB6qUMt' width=800>\n",
    "\n",
    "\n",
    "### Examples of raster data\n",
    "In this workshop, we'll be working with raster reflectance data from NEON AOP. We'll also be using gridded lidar data products. These include the **canopy height model (CHM), digital surface model (DSM), and digital terrain model (DTM)**\n",
    "<img src='https://drive.google.com/uc?export=view&id=1CgrBXd6otHGfMgYZS41d3h0Qja0FW0xl' width=400>\n",
    "\n",
    "### Common file formats\n",
    "- HDF5\n",
    "- GeoTIFF\n",
    "- ENVI\n",
    "- NetCDF\n",
    "\n",
    "### Common Python packages for raster data\n",
    "- Rasterio\n",
    "- Rioxarray\n",
    "- osgeo.gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSyUmi6-evqJ"
   },
   "source": [
    "## Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWgLnTs8evCV"
   },
   "source": [
    "[CU Boulder Earth Lab Tutorials](https://www.earthdatascience.org/) <br>\n",
    "[PyGIS - Open Source Spatial Programming and Remote Sensing](https://pygis.io/docs/a_intro.html) <br>\n",
    "[Spatial Thoughts tutorials and free online textbooks](https://courses.spatialthoughts.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRKZzvXRRCx8"
   },
   "source": [
    "# The National Ecological Observatory Network (NEON)\n",
    "\n",
    "NEON collects field data and airborne data across 81 sites freshwater and terrestrial sits in 20 ecoclimatic domains in the United States. <br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1rSfBId1EF-4Rb54Ex_KPRoZalmYb-Tbo' width=700> <br>\n",
    "Image from the National Ecological Observatory Network [(source)](https://www.neonscience.org/about/overview/design).\n",
    "\n",
    "\n",
    "You can download NEON data via the[NEON Data Portal](https://www.neonscience.org/data). You can learn more about NEON on [their website](https://www.neonscience.org/) or by clicking the image below to watch lectures and informational videos on their Youtube channel.\n",
    "\n",
    "[![NEON Youtube Channel](https://drive.google.com/uc?export=view&id=1Ok1voc99J6132SBapIQT38FUb7sktLTh)](https://youtu.be/39YrzpxVRF8 \"About NEON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC-99UkCT7Km"
   },
   "source": [
    "# Landscape Exchange Network for Socio-economic systems (LENS)\n",
    "LENS is a research coordination network created in July 2021 that includes researchers in the social and natural sciences who are working to broaden the participation of members of the community in research through collaborative projects that are supported by network meetings, three interdisciplinary working groups, and in-person and virtual workshops. The project is being funded by the National Science Foundation (NSF) Division of Environmental Biology in the BIO directorate under [grant #2054939](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2054939&HistoricalAwards=false).\n",
    "\n",
    "LENS has three objectives:\n",
    "\n",
    "\n",
    "1.   Characterize socio-environmental systems (SES) represented in the landscapes surveyed by the National Ecological Observation Network Airborne Observation Platform (NEON AOP) – (*Working Group 1*)\n",
    "2.   Develop strategies that support an effective translational ecology approach in the AOP landscapes – (*Working Group 2*)\n",
    "3.  Develop and communicate methods for using AOP data in SES research using the translational ecology approach – (*Working Group 3*)\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1yBmQcM8VqIYFWBERvi72juC7-ANFjAfU' width=500> <br>\n",
    "Figure from [Ordway et al. 2021](https://doi.org/10.1002/ecs2.3640): Diagram highlighting aspects of the NEON AOP that can contribute to understanding SES interactions and feedback processes in space and time using drought as an example. The difference between a pre-drought (A) and drought-impacted landscape (B) was chosen to highlight processes that may occur at various scales and in different locations across the landscape (a–g) over the 30-yr planned AOP lifespan. Aspects of socio-environmental change that can be better understood within an AOP landscape are highlighted in comparisons of (d–f) in panels A and B. For example, fine-scale resolution AOP data will allow researchers to understand drought and related human behavioral impacts on plant species composition and vegetation structure (d), canopy water content (e), and crop yields (f). In addition, AOP data combined with ancillary datasets enables analysis at larger spatial scales (C). The higher temporal resolution of other sensors can also be used to understand change within years at a NEON AOP site (C) and how pressures like reduced snowpack reverberate throughout social, governance, and ecological systems (A a–g → B a–g). Symbols used with permission from UMCES IAN Symbol library (ian.umces.edu).\n",
    "\n",
    "\n",
    "## Sources\n",
    "[LENS Website](https://www.lensrcn.org/home) <br>\n",
    "[Ordway et al. 2021](https://doi.org/10.1002/ecs2.3640)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}