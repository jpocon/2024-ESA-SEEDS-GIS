{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "OOOsqwESbbp-",
   "metadata": {
    "id": "OOOsqwESbbp-"
   },
   "source": [
    "## Mapping Fire Impacts - LiDAR-based\n",
    "\n",
    "#### Reflection exercises for each group:\n",
    "\n",
    "1. At the top of the Colab notebook that you share with everyone, please include your responses to the following questions:\n",
    "  - Choose at least 1 dataset to explore in more detail.\n",
    "  - What is the projection for this dataset?\n",
    "  - Where can you find more information on how the data were collected and how to interpret the metadata?\n",
    "  - Think about what data type each variable is.\n",
    "  - Is it vector or raster data? What properties exist for each dataset?\n",
    "  - What resolution are your data?\n",
    "\n",
    "2. At the top of the Colab notebook, write a short summary detailing the processing steps in the notebook and your results.\n",
    "  - Although these topics may be far removed from your own interests, how could these steps and analyses help in your own work?\n",
    "\n",
    "3. OPTIONAL - Expand your script by adding additional processing, analysis, or other data.\n",
    "\n",
    "As you're working through your exercise, **add code chunks to further document your scripts. Add additional comments to the code itself to clarify complicated processes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ja2tca8ge48L",
   "metadata": {
    "id": "Ja2tca8ge48L"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d747a5e6",
   "metadata": {
    "id": "d747a5e6"
   },
   "source": [
    "### Objectives\n",
    "After completing this exercise, you will be able to:\n",
    "\n",
    "* Use Python functions to programmatically download NEON AOP data from the API\n",
    "* Download and plot shapefiles and kmls (included as lidar metadata) to visualize coverage for a given year\n",
    "* Explore and plot the AOP discrete lidar point cloud contents in Python using the `laspy` package\n",
    "* Read in and plot the AOP L3 raster data products (CHM, DTM, DSM) in Python using the `rasterio` package\n",
    "\n",
    "Acknowledgement:\n",
    "> The National Ecological Observatory Network is a program sponsored by the National Science Foundation and operated under cooperative agreement by Battelle. This material is based in part upon work supported by the National Science Foundation through the NEON Program.\"\n",
    "\n",
    "This notebook was adapted from [Detecting changes in vegetation structure following fires using discrete-return LiDAR](https://github.com/NEONScience/NEON-Data-Skills/blob/main/tutorials/Python/AOP/Lidar/lidar-applications/lidar-wildfire/fire-effects-veg-structure-lidar.md), as well as the [Introduction to NEON Discrete Lidar Data in Python](https://github.com/NEONScience/NEON-Data-Skills/blob/main/tutorials/Python/AOP/Lidar/intro-lidar/intro_point_clouds_py/intro_discrete_point_clouds.md). The [NEON learning hub](https://www.neonscience.org/resources/learning-hub) is a great resource to learn more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BMisMePEKDXE",
   "metadata": {
    "id": "BMisMePEKDXE"
   },
   "source": [
    "<div id=\"ds-objectives\" markdown=\"1\">\n",
    "\n",
    "### Background\n",
    "\n",
    "The **Creek Fire** was a large wildfire that started in September 2020 in the Sierra National Forest, California and became one of the largest fires of the [2020 California wildfire season](https://en.wikipedia.org/wiki/2020_California_wildfires). This fire had burned into NEON's [Soaproot Saddle (SOAP)](https://www.neonscience.org/field-sites/soap) site by mid-September, causing a [high intensity burn over much of the site](https://www.neonscience.org/impact/observatory-blog/domain-digest-no-4-fire-field-sites-and-recovery) - standing trees became charcoal spires, shrubs and their root systems burned completely, the thick litter layer was incinerated, and the soil severely burned.\n",
    "\n",
    "The NEON Airborne Observation Platform (AOP) conducted aerial surveys over SOAP in 2019 and 2021, a year before and after the Creek Fire. This exercise aims to study the effects of fire on vegetation structure by comparing the lidar-derived relative height percentiles before and after the fire. In addition to the discrete return data, this tutorial uses a Digital Terrain Model (DTM) to determine the relative height of the discrete returns with respect to the ground.\n",
    "\n",
    "This Python tutorial is broken down into three parts:\n",
    "1. Read the NEON discrete-return lidar data ([DP1.30003.001](https://data.neonscience.org/data-products/DP1.30003.001)) and visualize the 3D lidar point cloud.\n",
    "2. Read the lidar-derived Digital Terrain Model ([DP3.30024.001](https://data.neonscience.org/data-products/DP3.30024.001)) in Python. Visualize the spatial extent of the lidar data used in this tutorial with that of the Creek Fire perimeter and the SOAP flight boundary.\n",
    "3. Calculate and compare the relative height percentiles of the discrete returns before and after the 2020 Creek Fire.\n",
    "\n",
    "NEON provides both discrete return and full-waveform LiDAR data for its field sites across the US. The discrete return LiDAR point cloud differs from the full waveform data as it is considerably smaller in file size and condenses the information into a small number of points per laser shot rather than many data bins (roughly 100 bins per laser shot in the full waveform data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76936410",
   "metadata": {
    "id": "76936410"
   },
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d662e",
   "metadata": {
    "id": "6b3d662e"
   },
   "outputs": [],
   "source": [
    "!pip install rasterio\n",
    "!pip install rioxarray\n",
    "!pip install laspy[lazrs,laszip]\n",
    "!pip install pyproj\n",
    "!pip install shapely\n",
    "!pip install seaborn\n",
    "!pip install geopandas\n",
    "#an acronym of “pip Install Packages” is today the standard tool for installing Python packages\n",
    "\n",
    "#import required packages\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import laspy, glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import pyproj\n",
    "from pyproj import Proj\n",
    "import shapely\n",
    "from shapely import Polygon, MultiPolygon  ## Try this if this line throws an error: from shapely.geometry import Polygon, MultiPolygon\n",
    "import seaborn as sns\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "from rasterio.plot import show\n",
    "import random\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fa0e0",
   "metadata": {
    "id": "083fa0e0"
   },
   "source": [
    "If you haven't saved the neon_aop_download_functions.py script locally, you can do that programmatically as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5f1e3",
   "metadata": {
    "id": "88a5f1e3"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/Python/AOP/aop_python_modules/neon_aop_download_functions.py\"\n",
    "response = requests.get(url)\n",
    "open(\"neon_aop_download_functions.py\", \"wb\").write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c67d7",
   "metadata": {
    "id": "f68c67d7"
   },
   "source": [
    "Now we'll pull in all the functions in the module **neon_aop_download_functions.py**.\n",
    "\n",
    "First make sure this script is saved in your working directory, which we can check by navigating into that directory, or by using `os.listdir`. If you haven't saved it in your working directory, you will need to provide the relative path to this script when importing.\n",
    "\n",
    "```python\n",
    "# check that script is saved in same folder:\n",
    "os.listdir()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b0d14",
   "metadata": {
    "id": "0d0b0d14"
   },
   "source": [
    "Once you've confirmed that the `neon_aop_download_functions script` is in your working directory, you can import the contents of this file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24803df",
   "metadata": {
    "id": "f24803df"
   },
   "outputs": [],
   "source": [
    "from neon_aop_download_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6072456",
   "metadata": {
    "id": "c6072456"
   },
   "source": [
    "Alternatively, if you'd like to see the contents of that file in your notebook, you can use the \"magic\" command `%load` as follows:\n",
    "\n",
    "```python\n",
    "%load neon_aop_download_functions.py\n",
    "```\n",
    "\n",
    "If you go this route, you will need to run the cell twice for the functions to be read into the notebook. The first run will load the functions into the cell, and the second run will load the functions into the working environment. This option of loading in the functions may be useful if you wish to modify the functions locally, eg. according to your specific workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ff1b6",
   "metadata": {
    "id": "f96ff1b6"
   },
   "source": [
    "Now that we've imported in all the required packages and functions, we can get started! First let's take a look at what exactly we've imported by using the magic command `%whos`. Since there is no variable explorer in Jupyter Notebooks, this is a quick way to see the contents (variables, functions, modules, etc.) that have been loaded in our current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85449f9c",
   "metadata": {
    "id": "85449f9c"
   },
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488f820",
   "metadata": {
    "id": "a488f820"
   },
   "source": [
    "#### Data Tip\n",
    "If you are unsure what a function or module does, you can find more information about the function in two ways:\n",
    "1. type `help(function_name)`, which will print out documentation below the cell, or\n",
    "2. type the function name followed by `?`, which will pop up the documentation at the bottom of your Jupyter Notebook window, and you can then exit out of it at your convenience\n",
    "\n",
    "```python\n",
    "help(requests)\n",
    "requests?\n",
    "```\n",
    "\n",
    "The functions loaded from the `neon_aop_download_functions.py` file also include similar documentation, so you can also use the help function with for user-defined functions, as long as the appropriate `docstrings` (comments) have been added.\n",
    "\n",
    "```python\n",
    "help(list_available_urls)\n",
    "list_available_urls?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe37451",
   "metadata": {
    "id": "8fe37451"
   },
   "source": [
    "Now that we've pulled in the packages needed for the first part of the tutorial, we can download a dataset from the NEON API.\n",
    "\n",
    "First we'll start by defining variables that specify the NEON data product ID (`dpID`), site, and year. You can change the site code to look at a different site of your interest (for more detailed information about the NEON sites, please check out the <a href=\"https://www.neonscience.org/field-sites/explore-field-sites\" target=\"_blank\">Explore Field Sites</a> webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd351604",
   "metadata": {
    "id": "fd351604"
   },
   "outputs": [],
   "source": [
    "dpID='DP1.30003.001'\n",
    "site = 'SOAP'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0454d",
   "metadata": {
    "id": "3cc0454d"
   },
   "source": [
    "We can use the function `list_available_urls` to see what data is available for this data product and site. This function requires two inputs: the data product ID `dpID` and the site ID, `site`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee92682",
   "metadata": {
    "id": "bee92682"
   },
   "outputs": [],
   "source": [
    "help(list_available_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36a921",
   "metadata": {
    "id": "cb36a921"
   },
   "outputs": [],
   "source": [
    "list_available_urls(dpID,site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ece75",
   "metadata": {
    "id": "9f4ece75"
   },
   "source": [
    "The AOP has only flown Puerto Rico (D04) once so far, in 2018. D04 is only on the AOP schedule every 4 years; the next campaign is scheduled for the fall of 2022, so new data is expected relatively soon!\n",
    "\n",
    "Next let's set up some paths where we can save our downloaded data. We'll start with downloading geospatial metadata in order to get a sense of the coverage (geographic extents) of the flight area. We'll make a folder for the shapefiles (shp), kml files (which may be useful for interactively looking at the site boundaries in Google Earth), and a folder for the actual lidar (laz) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4bd12",
   "metadata": {
    "id": "50b4bd12"
   },
   "outputs": [],
   "source": [
    "year='2021'\n",
    "year2='2019'\n",
    "data_root_path = './data/'+site+'/'+year+'/'\n",
    "data_root_path2 = './data/'+site+'/'+year2+'/'\n",
    "shp_path = data_root_path+'shp/'\n",
    "shp_path2 = data_root_path2+'shp/'\n",
    "kml_path = data_root_path+'kml/'\n",
    "kml_path2 = data_root_path2+'kml/'\n",
    "laz_path = data_root_path+'laz/'\n",
    "laz_path2 = data_root_path2+'laz/'\n",
    "print(data_root_path)\n",
    "print(data_root_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94078aaf",
   "metadata": {
    "id": "94078aaf"
   },
   "source": [
    "Next, let's take a look at the `download_aop_files` function, which we'll use to download the metadata and data that we want to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc8c76",
   "metadata": {
    "id": "37fc8c76"
   },
   "outputs": [],
   "source": [
    "help(download_aop_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4707a0",
   "metadata": {
    "id": "dc4707a0"
   },
   "source": [
    "The only required inputs for this function are the `product` and the `site`; optionally we can specify the `year`, the `download_folder` to save the files, and a `match_string` to download a subset of the data by a string. By default, the function will display the size of the files, and prompt the user to continue the download (by typing `y`); any other response will halt the download. This is to prevent an accidental download of a large volume of data.\n",
    "\n",
    "### Lidar Metadata\n",
    "\n",
    "We'll start by downloading the some of the metadata, including the pdf documentation, and shape files that provide geographic information corresponding to the data. Because AOP data can be pretty large for an entire site, and you may only need to work with a subset of the data for a given site, we recommend starting with downloading only the metadata.\n",
    "\n",
    "#### Lidar Documentation - QA pdfs\n",
    "\n",
    "AOP data provides summary pdf documents, which include important information about the sensors used to collect the lidar data, acquisition parameters, processing parameters, and QA information. When working with any AOP data, we recommend reviewing this documentation, as well as referencing the relevant ATBDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c18e6",
   "metadata": {
    "id": "c59c18e6"
   },
   "outputs": [],
   "source": [
    "#download the QA reports to the default download directory (./data)\n",
    "download_aop_files(dpID,site,year,match_string='.pdf',check_size=False)\n",
    "download_aop_files(dpID,site,year2,match_string='.pdf',check_size=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf610a",
   "metadata": {
    "id": "eeaf610a"
   },
   "source": [
    "Please take a look at these pdfs on your own time!\n",
    "\n",
    "#### Download and Plot Shapefile Boundaries\n",
    "\n",
    "There are summary shape files provided along with the lidar data for each site. These summary files end with `merged_tiles.shp/.shx`, so we can key off that string to download only the full boundary shape file. You could also download all of the individual `.shp` files for each data tile (L3 data is provided in 1km x 1km tiles), by using the match string `.shp`, or similarly all the `.kml` files, if you wanted to pull the data boundaries into Google Earth and explore more interactively.\n",
    "\n",
    "We'll download the merged_tiles shapefiles (you need to download both extensions .shp and .shx):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659fd5c",
   "metadata": {
    "id": "4659fd5c"
   },
   "outputs": [],
   "source": [
    "#download the full-boundary shape files\n",
    "download_aop_files(dpID,site,year,shp_path,'merged_tiles.shp',check_size=False)\n",
    "download_aop_files(dpID,site,year,shp_path,'merged_tiles.shx',check_size=False)\n",
    "\n",
    "download_aop_files(dpID,site,year2,shp_path2,'merged_tiles.shp',check_size=False)\n",
    "download_aop_files(dpID,site,year2,shp_path2,'merged_tiles.shx',check_size=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db05e2",
   "metadata": {
    "id": "66db05e2"
   },
   "source": [
    "We can see that these files have downloaded to the expected location by listing the contents of the `shp_path` directory that we've made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd4da0",
   "metadata": {
    "id": "dfcd4da0"
   },
   "outputs": [],
   "source": [
    "os.listdir(shp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OnMPxmvtBR6y",
   "metadata": {
    "id": "OnMPxmvtBR6y"
   },
   "outputs": [],
   "source": [
    "os.listdir(shp_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e41086",
   "metadata": {
    "id": "35e41086"
   },
   "source": [
    "We can see these files in the `shp` directory we created. Next, let's plot the boundary shapefile data using `geopandas` (imported as `gpd`) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae0192",
   "metadata": {
    "id": "51ae0192"
   },
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join(shp_path,'NEON_D17_SOAP_DPQA_2021_merged_tiles.shp'))\n",
    "gdf.plot();\n",
    "ax = plt.gca(); ax.ticklabel_format(style='plain')\n",
    "ax.set_title('AOP Coverage of ' + site + ' in ' + year);\n",
    "plt.xticks(rotation=90); #optionally rotate the xtick labels\n",
    "\n",
    "gdf = gpd.read_file(os.path.join(shp_path2,'2019_SOAP_4_merged_tiles.shp'))\n",
    "gdf.plot();\n",
    "ax = plt.gca(); ax.ticklabel_format(style='plain')\n",
    "ax.set_title('AOP Coverage of ' + site + ' in ' + year2);\n",
    "plt.xticks(rotation=90); #optionally rotate the xtick labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408d9c5",
   "metadata": {
    "id": "f408d9c5"
   },
   "outputs": [],
   "source": [
    "download_aop_files(dpID,site,year,kml_path,'full_boundary.kml',check_size=False)\n",
    "download_aop_files(dpID,site,year2,kml_path2,'full_boundary.kml',check_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82ea31",
   "metadata": {
    "id": "4c82ea31"
   },
   "outputs": [],
   "source": [
    "download_aop_files('DP1.30003.001',site,year,laz_path,match_string='293000_4100000_classified_point_cloud_colorized.laz')\n",
    "download_aop_files('DP1.30003.001',site,year2,laz_path2,match_string='293000_4100000_classified_point_cloud_colorized.laz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca9c93",
   "metadata": {
    "id": "0fca9c93"
   },
   "source": [
    "We can use `os.listdir` again to check that this file successfully downloaded to the expected location. Alternatively you could go into your file explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b928e99",
   "metadata": {
    "id": "5b928e99"
   },
   "outputs": [],
   "source": [
    "os.listdir(laz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986eBJbeBiYP",
   "metadata": {
    "id": "986eBJbeBiYP"
   },
   "outputs": [],
   "source": [
    "os.listdir(laz_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700edb1a",
   "metadata": {
    "id": "700edb1a"
   },
   "source": [
    "### Exploring point cloud data with `laspy`\n",
    "\n",
    "Now that we've successfully downloaded a laz (or zipped las) file, we can use the `laspy` package to read it in! We'll do that in the next line, reading the lidar file into the variable name `point_cloud`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cecd7",
   "metadata": {
    "id": "389cecd7"
   },
   "outputs": [],
   "source": [
    "# use os.path.join to get the full path of the laz file\n",
    "point_cloud_2021=os.path.join(laz_path,'NEON_D17_SOAP_DP1_293000_4100000_classified_point_cloud_colorized.laz')\n",
    "# read the laz file into a LasData object using laspy.read()\n",
    "las_2021=laspy.read(point_cloud_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pr_si2ZT8mqj",
   "metadata": {
    "id": "Pr_si2ZT8mqj"
   },
   "outputs": [],
   "source": [
    "# use os.path.join to get the full path of the laz file\n",
    "point_cloud_2019=os.path.join(laz_path2,'NEON_D17_SOAP_DP1_293000_4100000_classified_point_cloud_colorized.laz')\n",
    "# read the laz file into a LasData object using laspy.read()\n",
    "las_2019=laspy.read(point_cloud_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8595fdc",
   "metadata": {
    "id": "f8595fdc"
   },
   "source": [
    "Reading in the file with with laspy.read() reads in both the metadata and the raw point cloud data. We can print out the `point_cloud` variable to show some basic information about what we've read in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17107096",
   "metadata": {
    "id": "17107096"
   },
   "outputs": [],
   "source": [
    "las_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5TlAS6_HB3LR",
   "metadata": {
    "id": "5TlAS6_HB3LR"
   },
   "outputs": [],
   "source": [
    "las_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3a616",
   "metadata": {
    "id": "80a3a616"
   },
   "source": [
    "`point_format.dimension_names` show us the available information stored in this LasData object format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933ff02",
   "metadata": {
    "id": "9933ff02"
   },
   "outputs": [],
   "source": [
    "list(las_2021.point_format.dimension_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03622c",
   "metadata": {
    "id": "db03622c"
   },
   "source": [
    "In the next few cells, we can explore some of these variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bf427",
   "metadata": {
    "id": "ef9bf427"
   },
   "outputs": [],
   "source": [
    "las_2021.classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f454e",
   "metadata": {
    "id": "429f454e"
   },
   "source": [
    "Let's get the `set` of this `list` to see all the unique classification values in this file. This may take a little time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a847415d",
   "metadata": {
    "id": "a847415d"
   },
   "outputs": [],
   "source": [
    "set(list(las_2021.classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074e30d",
   "metadata": {
    "id": "d074e30d"
   },
   "source": [
    "We can see that there are a several unique classification values for this site.\n",
    "Las files have \"predefined classification schemes defined by the American Society for Photogrammetry and Remote Sensing (ASPRS)\". You can refer to the <a href=\"https://desktop.arcgis.com/en/arcmap/10.3/manage-data/las-dataset/lidar-point-classification.htm\" target=\"_blank\">ArcGIS documentation</a> for more details.\n",
    "\n",
    "The following table lists the LAS classification codes defined by ASPRS for these LAS versions:\n",
    "\n",
    "| Classification value | Meaning           |\n",
    "|---------------------|-------------------|\n",
    "| 0                   | Never classified  |\n",
    "| 1                   | Unassigned        |\n",
    "| 2                   | Ground            |\n",
    "| 3                   | Low Vegetation    |\n",
    "| 4                   | Medium Vegetation |\n",
    "| 5                   | High Vegetation   |\n",
    "| 6                   | Building          |\n",
    "| 7                   | Low Point         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359209a",
   "metadata": {
    "id": "f359209a"
   },
   "source": [
    "Next let's take a look at what we can consider to be the main data - the geographic loation of each point in the point cloud. This can be accessed either by `point_cloud.xyz`. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yPznURmqEOMO",
   "metadata": {
    "id": "yPznURmqEOMO"
   },
   "outputs": [],
   "source": [
    "## Function to import las file as a dataframe in python\n",
    "def las_to_df(las):\n",
    "  x = pd.DataFrame(np.array(las.x))\n",
    "  y = pd.DataFrame(np.array(las.y))\n",
    "  z = pd.DataFrame(np.array(las.z))\n",
    "  intensity = pd.DataFrame(np.array(las.intensity))\n",
    "  return_num = pd.DataFrame(np.array(las.return_number))\n",
    "  number_of_returns = pd.DataFrame(np.array(las.number_of_returns))\n",
    "  classification = pd.DataFrame(np.array(las.classification)) ## 0 - 31 as per ASPRS classification scheme\n",
    "\n",
    "  df = pd.concat([x, y, z, intensity, return_num, number_of_returns, classification], axis=1)\n",
    "  df.columns=[\"x\", \"y\", \"z\", \"intensity\", \"return_num\", \"number_of_returns\", \"classification\"]\n",
    "  return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nofK0L5qERbn",
   "metadata": {
    "id": "nofK0L5qERbn"
   },
   "outputs": [],
   "source": [
    "## Call the las_to_df function to extract the 2019 and 2021 data as separate dataframes\n",
    "point_cloud_df_2019 = las_to_df(las_2019)\n",
    "point_cloud_df_2021 = las_to_df(las_2021)\n",
    "point_cloud_df_2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NZcwJfSCFVKl",
   "metadata": {
    "id": "NZcwJfSCFVKl"
   },
   "outputs": [],
   "source": [
    "## Use np.stack to combine the X, Y and Z into one array\n",
    "## Convert (3 x n) array to (n x 3) using transpose\n",
    "point_data_2021 = np.stack([las_2021.X, las_2021.Y, las_2021.Z]).transpose()\n",
    "point_data_2019 = np.stack([las_2019.X, las_2019.Y, las_2019.Z]).transpose()\n",
    "print(\"Number of discrete returns in the 2021 point cloud file = %s\" %\"{:,}\".format(point_data_2021.shape[0]))\n",
    "print(\"Number of discrete returns in the 2019 point cloud file = %s\" %\"{:,}\".format(point_data_2019.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cffd09",
   "metadata": {
    "id": "77cffd09"
   },
   "source": [
    "There are > 2 million lidar points in this single 1km x 1km tile. For the rest of this exercise, we'll look at a random subset of these points, taking every100th point (you can change this subset factor, but when we visualize the data in a few steps, subsetting by a larger factor will speed up the time it takes to make the plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YCU3yot6FbF_",
   "metadata": {
    "id": "YCU3yot6FbF_"
   },
   "outputs": [],
   "source": [
    "factor=100\n",
    "point_data_2021_sub = point_data_2021[::factor]\n",
    "point_data_2019_sub = point_data_2019[::factor]\n",
    "print(\"Number of discrete returns in the 2021 point cloud subsample = %s\" %\"{:,}\".format(point_data_2021_sub.shape[0]))\n",
    "print(\"Number of discrete returns in the 2019 point cloud subsample = %s\" %\"{:,}\".format(point_data_2019_sub.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d301f59",
   "metadata": {
    "id": "8d301f59"
   },
   "source": [
    "These point clouds have been \"colorized\" by the camera RGB imagery. If you refer back to the dimension names, you can see there are a `red`, `green`, and `blue` attributes. We can pull these into a single array by using `np.vstack`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hYpMoje5FfVr",
   "metadata": {
    "id": "hYpMoje5FfVr"
   },
   "outputs": [],
   "source": [
    "colors_2021 = np.stack([las_2021.red, las_2021.green, las_2021.blue]).transpose()\n",
    "colors_2019 = np.stack([las_2019.red, las_2019.green, las_2019.blue]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0959b0",
   "metadata": {
    "id": "8e0959b0"
   },
   "source": [
    "These colors have been scaled to store the color at a higher resolution, accomodated by the camera, so we'll need to re-scale the values between 0-1 in order to use them in our plot. The code below does this re-scaling, and then subsets the color data to by same factor we used to subset the `xyz` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UbcRnuxDFhSe",
   "metadata": {
    "id": "UbcRnuxDFhSe"
   },
   "outputs": [],
   "source": [
    "colors_2021_normalized = (colors_2021 - np.min(colors_2021))/np.ptp(colors_2021)\n",
    "colors_2021_normalized_sub = colors_2021_normalized[::factor]\n",
    "## We are going to zip the R,G,B values together for plotting using the matplotlib package\n",
    "colors_2021_normalized_sub = list(zip(colors_2021_normalized_sub[:,0], colors_2021_normalized_sub[:,1], colors_2021_normalized_sub[:,2]))\n",
    "\n",
    "colors_2019_normalized = (colors_2019 - np.min(colors_2019))/np.ptp(colors_2019)\n",
    "colors_2019_normalized_sub = colors_2019_normalized[::factor]\n",
    "colors_2019_normalized_sub = list(zip(colors_2019_normalized_sub[:,0], colors_2019_normalized_sub[:,1], colors_2019_normalized_sub[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4b6eb",
   "metadata": {
    "id": "ebb4b6eb"
   },
   "source": [
    "### 3D Point Cloud Visualization\n",
    "Lastly, we can visualize this 3D data using matplotlib to see what the point cloud looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3uIHw9UEFjjC",
   "metadata": {
    "id": "3uIHw9UEFjjC"
   },
   "outputs": [],
   "source": [
    "# Plot the las data in 3D\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "## Plot the 2021 data\n",
    "ax.scatter(point_data_2021_sub[:,0], point_data_2021_sub[:,1], point_data_2021_sub[:,2], color=colors_2021_normalized_sub, s=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2h0Ajj0LFmpH",
   "metadata": {
    "id": "2h0Ajj0LFmpH"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(point_data_2019_sub[:,0], point_data_2019_sub[:,1], point_data_2019_sub[:,2], color=colors_2019_normalized_sub, s=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612175f7",
   "metadata": {
    "id": "612175f7"
   },
   "source": [
    "### Lidar Raster Data - DTM, DSM, and CHM\n",
    "\n",
    "Lastly, we'll take a look at some of the derived (Level-3, or L3) data products generated from this point cloud data. NEON generates 5 different derived L3 products from the discrete data, summarized below.\n",
    "\n",
    "In the last part of this lesson, we'll show how to read in and visualize the CHM, DTM, and DSM data using the Python package `rasterio`. First we'll import the package and sub-package that's used to display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16351133",
   "metadata": {
    "id": "16351133"
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.plot import show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbcb55a",
   "metadata": {
    "id": "9cbcb55a"
   },
   "source": [
    "In the next couple cells, we'll create a path to save the raster data, and then download the L3 CHM, DTM, and DSM geotiffs of the same tile as the point cloud file we downloaded earlier. You'll see the syntax here is the same, we are just using different data product IDs and match_strings with the appropriate extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76151b11",
   "metadata": {
    "id": "76151b11"
   },
   "outputs": [],
   "source": [
    "l3_path = data_root_path+'L3/'\n",
    "l3_path2 = data_root_path2+'L3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed14b1",
   "metadata": {
    "id": "4eed14b1"
   },
   "outputs": [],
   "source": [
    "download_aop_files('DP3.30015.001',site,year,l3_path,match_string='293000_4100000_CHM.tif',check_size=False)\n",
    "download_aop_files('DP3.30024.001',site,year,l3_path,match_string='293000_4100000_DTM.tif',check_size=False)\n",
    "download_aop_files('DP3.30024.001',site,year,l3_path,match_string='293000_4100000_DSM.tif',check_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3_-kHZw5GUyh",
   "metadata": {
    "id": "3_-kHZw5GUyh"
   },
   "outputs": [],
   "source": [
    "download_aop_files('DP3.30015.001',site,year2,l3_path2,match_string='293000_4100000_CHM.tif',check_size=False)\n",
    "download_aop_files('DP3.30024.001',site,year2,l3_path2,match_string='293000_4100000_DTM.tif',check_size=False)\n",
    "download_aop_files('DP3.30024.001',site,year2,l3_path2,match_string='293000_4100000_DSM.tif',check_size=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b7450",
   "metadata": {
    "id": "011b7450"
   },
   "source": [
    "Next we'll read these in, using `rasterio`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33bffea",
   "metadata": {
    "id": "c33bffea"
   },
   "outputs": [],
   "source": [
    "chm = rasterio.open(os.path.join(l3_path,'NEON_D17_SOAP_DP3_293000_4100000_CHM.tif'))\n",
    "dtm = rasterio.open(os.path.join(l3_path,'NEON_D17_SOAP_DP3_293000_4100000_DTM.tif'))\n",
    "dsm = rasterio.open(os.path.join(l3_path,'NEON_D17_SOAP_DP3_293000_4100000_DSM.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CsUpXD78GnKj",
   "metadata": {
    "id": "CsUpXD78GnKj"
   },
   "outputs": [],
   "source": [
    "chm2019 = rasterio.open(os.path.join(l3_path2,'NEON_D17_SOAP_DP3_293000_4100000_CHM.tif'))\n",
    "dtm2019 = rasterio.open(os.path.join(l3_path2,'NEON_D17_SOAP_DP3_293000_4100000_DTM.tif'))\n",
    "dsm2019 = rasterio.open(os.path.join(l3_path2,'NEON_D17_SOAP_DP3_293000_4100000_DSM.tif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a689b",
   "metadata": {
    "id": "730a689b"
   },
   "source": [
    "And finally, we can plot the data using the rasterio `show` function that we imported earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b610fb2",
   "metadata": {
    "id": "2b610fb2"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "show((chm), ax=ax1, title='CHM');\n",
    "show((dtm), ax=ax2, title='DTM');\n",
    "show((dsm), ax=ax3, title='DSM');\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ty4lONrUGsHP",
   "metadata": {
    "id": "Ty4lONrUGsHP"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "show((chm2019), ax=ax1, title='CHM');\n",
    "show((dtm2019), ax=ax2, title='DTM');\n",
    "show((dsm2019), ax=ax3, title='DSM');\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ELRqx4zNJTIF",
   "metadata": {
    "id": "ELRqx4zNJTIF"
   },
   "source": [
    "# Lidar Raster Data (group notes)\n",
    "* DSM, DTM and CHM are derived (Level-3, or L3) data products generated from this point cloud data\n",
    "* Package - rasterio\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=10xcPfqTQVfzfNlaC6M72KoyVldwLzanu' width=800> Image credit: [OpenTopography](https://opentopography.org/lidar_basics)\n",
    "\n",
    "DSM\n",
    "* digital surface model\n",
    "* a three-dimensional representation of the upper elevations of a landscape\n",
    "* includes the bare surface and above-ground features like vegetation, buildings, and other structures.\n",
    "\n",
    "DTM\n",
    "* digital terrain model\n",
    "* a three-dimensional representation of the bare ground surface\n",
    "* natural and built features (i.e., vegetation and buildings) are removed from the landscape to show just the topography\n",
    "\n",
    "CHM\n",
    "* Canopy height models\n",
    "* measurement of trees, vegetation, buildings, and other structures above the bare ground surface.\n",
    "* CHM = DSM-DTM\n",
    "\n",
    "2019 vs 2021 analysis\n",
    "* dowload data\n",
    "```python\n",
    "# dowlnoad data (example code)\n",
    "download_aop_files('DP3.30015.001',site,year2,l3_path2,match_string='293000_4100000_CHM.tif',check_size=False)\n",
    "```\n",
    "```python\n",
    "# read the files using rasterio (example code)\n",
    "chm = rasterio.open(os.path.join(l3_path,'NEON_D17_SOAP_DP3_293000_4100000_CHM.tif'))\n",
    "dtm = rasterio.open(os.path.join(l3_path,'NEON_D17_SOAP_DP3_293000_4100000_DTM.tif'))\n",
    "dsm = rasterio.open(os.path.join(l3_path,'NEON_D17_SOAP_DP3_293000_4100000_DSM.tif'))\n",
    "```\n",
    "```python\n",
    "# plot the data (example code)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "show((chm), ax=ax1, title='CHM');\n",
    "show((dtm), ax=ax2, title='DTM');\n",
    "show((dsm), ax=ax3, title='DSM');\n",
    "plt.show;\n",
    "```\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=10xcov5Hk4sPvnDu2RwVTrQjm1ICZ3VlS' width=800> Image credit: [2019 vs 2021; note the darker areas in the 2021 plot]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dj4M9QygkpnB",
   "metadata": {
    "id": "dj4M9QygkpnB"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_cZEh7g0KoV5",
   "metadata": {
    "id": "_cZEh7g0KoV5"
   },
   "source": [
    "### Calculate and compare the relative height percentiles of the discrete returns before (2019) and after (2021) the 2020 Creek fire.\n",
    "\n",
    "We will now be calculating pixel-wise height percentiles ($20^{th}$, $30^{th}$, $50^{th}$, $75^{th}$, and $90^{th}$ percentile heights) of the discrete returns relative to the ground (DTM) for the years 2019 and 2021. To ensure that we have sufficient number of lidar returns per pixel for calculating percentiles, we will be calculating these metrics for every 10m pixel on the ground. So, our tasks here would be to:\n",
    "\n",
    "1) Calculate heights of discrete returns with repect to the 1m DTM. Since the ground heights do not change from year to year, we will be using the 2021 DTM to calculate heights of the discrete returns with respect to ground for both the years 2019 and 2021. The 2020 fire event would have likely cleared up some of the low vegetation and ground litter, thereby improving ground detection post-fire in 2021. Therefore, the 2021 DTM may be a more accurate representation of the true ground compared to the 2019 DTM.\n",
    "2) Create a 10m spatial resolution raster grid and assign a unique id to each 10m pixel.\n",
    "3) Group all discrete returns based on the 10m pixel they fall into.\n",
    "4) Calculate height percentiles ($20^{th}$, $30^{th}$, $50^{th}$, $75^{th}$, and $90^{th}$ percentile heights) relative to the ground for each 10m pixel.\n",
    "5) Compare the distribution of heights for each height percentile before and after the fire.\n",
    "\n",
    "### 1. Discrete-return heights relative to 2021 DTM\n",
    "\n",
    "To extract the ground elevation associated with each discrete return, we will be sampling values from the 2021 DTM raster at the x,y locations of each discrete return. We can then subtract the ground elevation from the height of the discrete return (z) to get the height of the return above ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VrIapx5tNgjD",
   "metadata": {
    "id": "VrIapx5tNgjD"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kMOk1QB2HBqj",
   "metadata": {
    "id": "kMOk1QB2HBqj"
   },
   "outputs": [],
   "source": [
    "## zip all x and y coordinates together for sampling rasters in the next cell\n",
    "coords_2019 = [(x,y) for x, y in zip(point_cloud_df_2019[\"x\"], point_cloud_df_2019[\"y\"])]\n",
    "coords_2021 = [(x,y) for x, y in zip(point_cloud_df_2021[\"x\"], point_cloud_df_2021[\"y\"])]\n",
    "\n",
    "print(\"Number of discrete returns in 2021 data = %d\" %len(coords_2021))\n",
    "print(\"Number of discrete returns in 2019 data = %d\" %len(coords_2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lgfve0p2HF4v",
   "metadata": {
    "id": "lgfve0p2HF4v"
   },
   "outputs": [],
   "source": [
    "## Set a seed value to make the code output reproducible\n",
    "seed_val = 0\n",
    "random.seed(seed_val)\n",
    "\n",
    "## Randomly sample 1 million rows from the 2019 and 2021 point cloud dataframes\n",
    "point_cloud_df_2019_sub = point_cloud_df_2019.sample(n=1000000).reset_index(drop=True)\n",
    "point_cloud_df_2021_sub = point_cloud_df_2021.sample(n=1000000).reset_index(drop=True)\n",
    "\n",
    "## zip all x and y coordinates together for sampling rasters in the next step\n",
    "coords_2019_sub = [(x,y) for x, y in zip(point_cloud_df_2019_sub[\"x\"], point_cloud_df_2019_sub[\"y\"])]\n",
    "coords_2021_sub = [(x,y) for x, y in zip(point_cloud_df_2021_sub[\"x\"], point_cloud_df_2021_sub[\"y\"])]\n",
    "\n",
    "## Sample the raster using \"rasterio.sample.sample_gen()\"\n",
    "\n",
    "## Sample the 2021 DTM raster for ground elevation at each (x,y) location of the 2021 discrete returns\n",
    "dtm_vals_2021 = pd.DataFrame(list(rasterio.sample.sample_gen(dtm, coords_2021_sub)))\n",
    "\n",
    "## Sample the 2021 DTM raster for ground elevation at each (x,y) location of the 2019 discrete returns\n",
    "## The ground elevations shouldn't change from year to year, so it is OK to use the 2021 DTM for both years.\n",
    "## Moreover, the fire event in 2020 likely cleared the understory vegetation, and therefore, the ground elevations might be more accurate in 2021\n",
    "dtm_vals_2019 = pd.DataFrame(list(rasterio.sample.sample_gen(dtm2019, coords_2019_sub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rqf0VCBKHxO-",
   "metadata": {
    "id": "Rqf0VCBKHxO-"
   },
   "outputs": [],
   "source": [
    "## Merge the point cloud dataframe (point_cloud_df_20xx_sub) with the DTM ground elevations extracted (dtm_vals_20xx)\n",
    "## Calculate the relative height of each return with respect to ground (\"discrete_ret_ht_above_ground\")\n",
    "\n",
    "## 2019\n",
    "df_2019 = pd.concat([point_cloud_df_2019_sub, dtm_vals_2019], axis=1)\n",
    "df_2019.columns = [\"x\", \"y\", \"z\", \"intensity\", \"return_num\", \"number_of_returns\", \"classification\", \"ground_elevation\"]\n",
    "df_2019[\"discrete_ret_ht_above_ground\"] = df_2019[\"z\"] - df_2019[\"ground_elevation\"]\n",
    "\n",
    "## Do the same for 2021 data as well\n",
    "df_2021 = pd.concat([point_cloud_df_2021_sub, dtm_vals_2021], axis=1)\n",
    "df_2021.columns = [\"x\", \"y\", \"z\", \"intensity\", \"return_num\", \"number_of_returns\", \"classification\", \"ground_elevation\"]\n",
    "df_2021[\"discrete_ret_ht_above_ground\"] = df_2021[\"z\"] - df_2021[\"ground_elevation\"]\n",
    "df_2021.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u60h7NUkK5ao",
   "metadata": {
    "id": "u60h7NUkK5ao"
   },
   "source": [
    "### 2. Create a 10m spatial resolution raster grid and assign a unique id to each 10m pixel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O3GiiY0QHz-S",
   "metadata": {
    "id": "O3GiiY0QHz-S"
   },
   "outputs": [],
   "source": [
    "## Create a (100 x 100) dataframe with each cell having a unique value between 0 and 10,000\n",
    "ids = pd.DataFrame(data=np.arange(0, 100*100).reshape(100,100), index=np.arange(0,100), columns=np.arange(0,100))\n",
    "\n",
    "## Create a scaled transform\n",
    "scaled_transform = dtm.transform * dtm.transform.scale((10),(10))\n",
    "\n",
    "## Using rasterio, save the dataframe as a 10m raster in the tif format\n",
    "with rasterio.open(\n",
    "    data_root_path + '/DTM_10m_unique_id.tif',\n",
    "    'w',\n",
    "    driver='GTiff',\n",
    "    height=ids.shape[0],\n",
    "    width=ids.shape[1],\n",
    "    count=1,\n",
    "    dtype=np.dtype(np.int32),\n",
    "    crs=dtm.crs,\n",
    "    transform=scaled_transform,\n",
    ") as dst:\n",
    "    dst.write(ids, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJavO8fkIbTF",
   "metadata": {
    "id": "MJavO8fkIbTF"
   },
   "outputs": [],
   "source": [
    "## Use rioxarray again to plot the newly created 10m raster\n",
    "raster_10m = rioxarray.open_rasterio(data_root_path + \"DTM_10m_unique_id.tif\")\n",
    "print(raster_10m)\n",
    "raster_10m.plot()\n",
    "plt.xlabel(\"Easting (m)\")\n",
    "plt.ylabel(\"Northing (m)\")\n",
    "plt.title(\"10m raster with unique pixel ids\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vch9rXegK_A-",
   "metadata": {
    "id": "Vch9rXegK_A-"
   },
   "source": [
    "### 3. Group all discrete returns based on the 10m pixel they fall into.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "os_T55T5IiZu",
   "metadata": {
    "id": "os_T55T5IiZu"
   },
   "outputs": [],
   "source": [
    "## Here, we assign unique ids created in the previous cell to each discrete return\n",
    "## Sample the unique ids from the 10m raster for each discrete return for the years 2019 and 2021\n",
    "## This sampling step might again take a few minutes\n",
    "\n",
    "raster_10m = rasterio.open(data_root_path + \"DTM_10m_unique_id.tif\")\n",
    "## For the year 2021\n",
    "dtm_id_vals_2021 = pd.DataFrame(list(rasterio.sample.sample_gen(raster_10m, coords_2021_sub)))\n",
    "\n",
    "## For the year 2019\n",
    "dtm_id_vals_2019 = pd.DataFrame(list(rasterio.sample.sample_gen(raster_10m, coords_2019_sub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TI598aZ0JD8m",
   "metadata": {
    "id": "TI598aZ0JD8m"
   },
   "outputs": [],
   "source": [
    "## Merge the unique ids extracted above with the discrete return dataframe (df_20xx) created earlier\n",
    "\n",
    "## Update 2019 df\n",
    "df_2019_with_ids = pd.concat([df_2019, dtm_id_vals_2019], axis=1)\n",
    "df_2019_with_ids.columns = [\"x\", \"y\", \"z\", \"intensity\", \"return_num\", \"number_of_returns\", \"classification\",\n",
    "                   \"ground_elevation\", \"discrete_ret_ht_above_ground\", \"uniq_id\"]\n",
    "df_2019_with_ids = df_2019_with_ids[df_2019_with_ids[\"ground_elevation\"] > -9999.0].reset_index(drop=True)\n",
    "\n",
    "## Update 2021 df\n",
    "df_2021_with_ids = pd.concat([df_2021, dtm_id_vals_2021], axis=1)\n",
    "df_2021_with_ids.columns = [\"x\", \"y\", \"z\", \"intensity\", \"return_num\", \"number_of_returns\", \"classification\",\n",
    "                            \"ground_elevation\", \"discrete_ret_ht_above_ground\", \"uniq_id\"]\n",
    "df_2021_with_ids = df_2021_with_ids[df_2021_with_ids[\"ground_elevation\"] > -9999.0].reset_index(drop=True)\n",
    "df_2021_with_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l00vqzNIJF0O",
   "metadata": {
    "id": "l00vqzNIJF0O"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "return_counts_per_10m_pixel_2019 = np.unique(df_2019_with_ids[\"uniq_id\"].sort_values(), return_counts=True)[1]\n",
    "return_counts_per_10m_pixel_2021 = np.unique(df_2021_with_ids[\"uniq_id\"].sort_values(), return_counts=True)[1]\n",
    "plt.hist(return_counts_per_10m_pixel_2019, bins=np.arange(0,400,10), density=True, alpha=0.5, color=\"blue\")\n",
    "plt.hist(return_counts_per_10m_pixel_2021, bins=np.arange(0,400,10), density=True, alpha=0.5, color=\"red\")\n",
    "plt.xlabel(\"# discrete returns per 10m pixel\", labelpad=10)\n",
    "plt.ylabel(\"Density\", labelpad=10)\n",
    "plt.title(\"Distribution of # of discrete returns per 10m pixel\", fontsize=12, pad=10)\n",
    "\n",
    "legend_elements = [Patch(facecolor=\"blue\", edgecolor=\"blue\", label='2019', alpha=0.5),\n",
    "                  Patch(facecolor=\"red\", edgecolor=\"red\", label='2021', alpha=0.5)]\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uOA7fTqTLEVC",
   "metadata": {
    "id": "uOA7fTqTLEVC"
   },
   "source": [
    "### 4. Calculate height percentiles of discrete returns (20th, 30th, 50th, 75th, and 90th) for each 10m pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eIEVsYHBJIX4",
   "metadata": {
    "id": "eIEVsYHBJIX4"
   },
   "outputs": [],
   "source": [
    "## Subset the dataframes to include only the relevant columns\n",
    "df_2019_sub = pd.concat([df_2019_with_ids[\"x\"], df_2019_with_ids[\"y\"], df_2019_with_ids[\"uniq_id\"], df_2019[\"discrete_ret_ht_above_ground\"]], axis=1)\n",
    "df_2021_sub = pd.concat([df_2021_with_ids[\"x\"], df_2021_with_ids[\"y\"], df_2021_with_ids[\"uniq_id\"], df_2021[\"discrete_ret_ht_above_ground\"]], axis=1)\n",
    "\n",
    "## Calculate the number of returns per 10 m pixel\n",
    "return_counts_per_10m_pixel_2019 = pd.DataFrame(np.unique(df_2019_sub[\"uniq_id\"].sort_values(), return_counts=True)).transpose()\n",
    "return_counts_per_10m_pixel_2021 = pd.DataFrame(np.unique(df_2021_sub[\"uniq_id\"].sort_values(), return_counts=True)).transpose()\n",
    "return_counts_per_10m_pixel_2019.columns = return_counts_per_10m_pixel_2021.columns = [\"uniq_id\", \"number_of_returns\"]\n",
    "\n",
    "## Select only those 10m pixels which have greater than 50 returns\n",
    "return_counts_per_10m_pixel_2019 = return_counts_per_10m_pixel_2019[return_counts_per_10m_pixel_2019[\"number_of_returns\"] > 50].reset_index(drop=True)\n",
    "return_counts_per_10m_pixel_2021 = return_counts_per_10m_pixel_2021[return_counts_per_10m_pixel_2021[\"number_of_returns\"] > 50].reset_index(drop=True)\n",
    "valid_10m_pixels_2019 = return_counts_per_10m_pixel_2019[\"uniq_id\"]\n",
    "valid_10m_pixels_2021 = return_counts_per_10m_pixel_2021[\"uniq_id\"]\n",
    "\n",
    "## Update df_sub to include only valid 10m pixels (returns > 50)\n",
    "df_2019_sub = df_2019_sub[df_2019_sub[\"uniq_id\"].isin(valid_10m_pixels_2019)].reset_index(drop=True)\n",
    "df_2021_sub = df_2021_sub[df_2021_sub[\"uniq_id\"].isin(valid_10m_pixels_2021)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ioti3-JGJKvd",
   "metadata": {
    "id": "Ioti3-JGJKvd"
   },
   "outputs": [],
   "source": [
    "## For each unique id (uniq_id), calculate relative height percentiles (20th, 30th, 50th, 75th and 90th)\n",
    "\n",
    "## First define a function to calculate the percentile and return a percentile dataframe\n",
    "def calc_percentile(df,percentile):\n",
    "    ptile_df = pd.DataFrame(df.groupby(by=[\"uniq_id\"])[\"discrete_ret_ht_above_ground\"].quantile(percentile).reset_index(drop=True))\n",
    "    ptile_df[\"RH_ptile\"] = percentile*100\n",
    "    ptile_df.columns = [\"height\", \"RH_ptile\"]\n",
    "    return ptile_df\n",
    "\n",
    "ptile_20_2019 = calc_percentile(df_2019_sub,0.2)\n",
    "ptile_30_2019 = calc_percentile(df_2019_sub,0.3)\n",
    "ptile_50_2019 = calc_percentile(df_2019_sub,0.5)\n",
    "ptile_75_2019 = calc_percentile(df_2019_sub,0.75)\n",
    "ptile_90_2019 = calc_percentile(df_2019_sub,0.9)\n",
    "\n",
    "ptile_20_2021 = calc_percentile(df_2021_sub,0.2)\n",
    "ptile_30_2021 = calc_percentile(df_2021_sub,0.3)\n",
    "ptile_50_2021 = calc_percentile(df_2021_sub,0.5)\n",
    "ptile_75_2021 = calc_percentile(df_2021_sub,0.75)\n",
    "ptile_90_2021 = calc_percentile(df_2021_sub,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zg-Peou4JNDJ",
   "metadata": {
    "id": "Zg-Peou4JNDJ"
   },
   "outputs": [],
   "source": [
    "rh_percentiles_2019 = pd.concat([ptile_20_2019, ptile_30_2019, ptile_50_2019, ptile_75_2019, ptile_90_2019], axis=0, ignore_index=True)\n",
    "rh_percentiles_2019[\"year\"] = 2019\n",
    "\n",
    "rh_percentiles_2021 = pd.concat([ptile_20_2021, ptile_30_2021, ptile_50_2021, ptile_75_2021, ptile_90_2021], axis=0, ignore_index=True)\n",
    "rh_percentiles_2021[\"year\"] = 2021\n",
    "\n",
    "## Combine 2019 and 2021 dataframes\n",
    "rh_percentiles_combined = pd.concat([rh_percentiles_2019, rh_percentiles_2021], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O-DdYK2eLOyO",
   "metadata": {
    "id": "O-DdYK2eLOyO"
   },
   "source": [
    "### 5. Compare the distribution of height percentiles for the years 2019 and 2021.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3iIfQICRJOCm",
   "metadata": {
    "id": "3iIfQICRJOCm"
   },
   "outputs": [],
   "source": [
    "## Compare boxplots\n",
    "plt.subplots(figsize=(8,5))\n",
    "sns.boxplot(x=rh_percentiles_combined[\"RH_ptile\"], y=rh_percentiles_combined[\"height\"], hue=rh_percentiles_combined[\"year\"],\n",
    "           fliersize=0, whis=[5,95])\n",
    "plt.ylim(0,15)\n",
    "plt.xlabel(\"Height percentile\", fontsize=12)\n",
    "plt.ylabel(\"Heights (in m)\", fontsize=12)\n",
    "plt.title(\"Relative Height Percentiles before (2019) and after (2021) the 2020 Creek Fire\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HlIUUE_GfBmP",
   "metadata": {
    "id": "HlIUUE_GfBmP"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007c680",
   "metadata": {
    "id": "3007c680"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "* **L1 Point Clouds (.laz)**: If you'd like to continue exploring the point cloud data in Python using `laspy`, <a href=\"https://laspy.readthedocs.io/en/latest/complete_tutorial.html\" target=\"_blank\"> laspy website </a> has some nice examples you can follow, now that you know how to download NEON point cloud data and read it into Python.\n",
    "* **L3 Rasters (.tif)**: Refer to the <a href=\"https://rasterio.readthedocs.io/en/latest/\" target=\"_blank\"> rasterio documentation </a> for more options on plotting, and beyond in rasterio.\n",
    "\n",
    "### Python and Beyond - Other Options for working with Point Cloud Data\n",
    "\n",
    "There are also a number of open-source tools for working with point-cloud data. Python may not be the best option for developing more rigourous processing workflows, for example. The resources below show some other recommended tools that can be integrated with Python for your analysis:\n",
    "\n",
    "* <a href=\"https://rapidlasso.com/lastools/\" target=\"_blank\">LAStools</a>\n",
    "* <a href=\"https://pdal.io/en/stable/\" target=\"_blank\">PDAL (Point Data Abstraction Library)</a>\n",
    "* <a href=\"https://plas.io/\" target=\"_blank\">plas.io (free, interactive, web-based point cloud visualization)</a>\n",
    "* <a href=\"https://r-lidar.github.io/lidRbook/\" target=\"_blank\"> lidR (R package for point cloud data)</a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}